---
sidebar_label: 07.leader-election
sidebar_position: 20
title: 07.leader-election
---

> å¦‚æœä½ æ˜¯ä¸€ä¸ªæƒ³äº†è§£é¢†å¯¼è€…é€‰ä¸¾ä»¥åŠåœ¨ Kubernetes ä¸­å¦‚ä½•å·¥ä½œçš„äººï¼Œé‚£ä¹ˆæˆ‘å¸Œæœ›ä½ è§‰å¾—è¿™ç¯‡æ–‡ç« æœ‰ç”¨ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºé«˜å¯ç”¨æ€§ç³»ç»Ÿä¸­é¢†å¯¼è€…é€‰ä¸¾èƒŒåçš„æƒ³æ³•ï¼Œå¹¶æ¢ç´¢ kubernetes/client-go åº“ï¼Œä»¥ä¾¿åœ¨ Kubernetes æ§åˆ¶å™¨çš„ä¸Šä¸‹æ–‡ä¸­ç†è§£å®ƒã€‚

### ä»‹ç»
è¿‘å¹´æ¥ï¼Œéšç€å¯¹å¯é ç³»ç»Ÿå’ŒåŸºç¡€è®¾æ–½çš„éœ€æ±‚å¢åŠ ï¼Œâ€œé«˜å¯ç”¨æ€§â€ä¸€è¯è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ï¼Œé«˜å¯ç”¨æ€§é€šå¸¸æ¶‰åŠæœ€å¤§åŒ–æ­£å¸¸è¿è¡Œæ—¶é—´å’Œä½¿ç³»ç»Ÿå…·æœ‰å®¹é”™èƒ½åŠ›ã€‚é«˜å¯ç”¨æ€§ä¸­å¸¸è§çš„åšæ³•æ˜¯ä½¿ç”¨å†—ä½™æ¥æœ€å¤§ç¨‹åº¦åœ°å‡å°‘å•ç‚¹æ•…éšœã€‚å‡†å¤‡ç³»ç»Ÿå’ŒæœåŠ¡ä»¥å®ç°å†—ä½™å¯èƒ½å°±åƒåœ¨è´Ÿè½½å¹³è¡¡å™¨åé¢éƒ¨ç½²æ›´å¤šå‰¯æœ¬ä¸€æ ·ç®€å•ã€‚å°½ç®¡è¿™æ ·çš„é…ç½®å¯ä»¥é€‚ç”¨äºè®¸å¤šåº”ç”¨ç¨‹åºï¼Œä½†æŸäº›ç”¨ä¾‹éœ€è¦è·¨å‰¯æœ¬è¿›è¡Œä»”ç»†åè°ƒæ‰èƒ½ä½¿ç³»ç»Ÿæ­£å¸¸å·¥ä½œ

ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­æ˜¯å½“ Kubernetes æ§åˆ¶å™¨éƒ¨ç½²ä¸ºå¤šä¸ªå®ä¾‹æ—¶ã€‚ä¸ºäº†é˜²æ­¢ä»»ä½•æ„å¤–è¡Œä¸ºï¼Œé¢†å¯¼è€…é€‰ä¸¾è¿‡ç¨‹å¿…é¡»ç¡®ä¿åœ¨å‰¯æœ¬ä¸­é€‰å‡ºä¸€ä¸ªé¢†å¯¼è€…ï¼Œå¹¶ä¸”æ˜¯å”¯ä¸€ä¸»åŠ¨åè°ƒé›†ç¾¤çš„é¢†å¯¼è€…ã€‚å…¶ä»–å®ä¾‹åº”ä¿æŒéæ´»åŠ¨çŠ¶æ€ï¼Œä½†å·²å‡†å¤‡å¥½åœ¨é¢†å¯¼è€…å®ä¾‹å‘ç”Ÿæ•…éšœæ—¶æ¥ç®¡

### Kubernetes ä¸­çš„é¢†å¯¼è€…é€‰ä¸¾

Kubernetes ä¸­çš„é¢†å¯¼è€…é€‰ä¸¾è¿‡ç¨‹å¾ˆç®€å•ã€‚å®ƒä»åˆ›å»ºé”å®šå¯¹è±¡å¼€å§‹ï¼Œå…¶ä¸­é¢†å¯¼è€…å®šæœŸæ›´æ–°å½“å‰æ—¶é—´æˆ³ï¼Œä½œä¸ºé€šçŸ¥å…¶ä»–å‰¯æœ¬æœ‰å…³å…¶é¢†å¯¼çš„ä¸€ç§æ–¹å¼ã€‚è¿™ä¸ªé”å¯¹è±¡å¯ä»¥æ˜¯ Lease ã€ ConfigMap æˆ– Endpoint ï¼Œä¹ŸæŒæœ‰å½“å‰é¢†å¯¼è€…çš„èº«ä»½ã€‚å¦‚æœé¢†å¯¼è€…æœªèƒ½åœ¨ç»™å®šçš„æ—¶é—´é—´éš”å†…æ›´æ–°æ—¶é—´æˆ³ï¼Œåˆ™å‡å®šå®ƒå·²å´©æºƒï¼Œè¿™æ˜¯éæ´»åŠ¨å‰¯æœ¬é€šè¿‡ä½¿ç”¨å…¶èº«ä»½æ›´æ–°é”æ¥ç«ç›¸è·å–é¢†å¯¼æƒæ—¶ã€‚æˆåŠŸè·å¾—é”çš„ Pod å°†æˆä¸ºæ–°çš„é¢†å¯¼è€…

![](https://raw.githubusercontent.com/mouuii/picture/master/1_ea0yTtafuGKug1JdcN1YGg.webp)


åœ¨æˆ‘ä»¬å¤„ç†ä»»ä½•ä»£ç ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆçœ‹çœ‹è¿™ä¸ªè¿‡ç¨‹çš„å®é™…æ•ˆæœï¼

ç¬¬ä¸€æ­¥æ˜¯å»ºç«‹ä¸€ä¸ªæœ¬åœ°çš„ Kubernetes é›†ç¾¤ã€‚æˆ‘å°†ä½¿ç”¨ KinD ï¼Œä½†è¯·éšæ„é€‰æ‹©æ‚¨é€‰æ‹©çš„æœ¬åœ° k8s å‘è¡Œç‰ˆã€‚

```go
$ kind create cluster
Creating cluster "kind" ...
 âœ“ Ensuring node image (kindest/node:v1.21.1) ğŸ–¼
 âœ“ Preparing nodes ğŸ“¦
 âœ“ Writing configuration ğŸ“œ
 âœ“ Starting control-plane ğŸ•¹ï¸
 âœ“ Installing CNI ğŸ”Œ
 âœ“ Installing StorageClass ğŸ’¾
Set kubectl context to "kind-kind"
You can now use your cluster with:
kubectl cluster-info --context kind-kind
Not sure what to do next? ğŸ˜…  Check out https://kind.sigs.k8s.io/docs/user/quick-start/
```


æˆ‘ä»¬å°†ä½¿ç”¨çš„ç¤ºä¾‹åº”ç”¨ç¨‹åºå¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/mayankshah1607/k8s-leader-election)æ‰¾åˆ°ã€‚å®ƒä½¿ç”¨ kubernetes/client-go æ¥æ‰§è¡Œé¢†å¯¼è€…é€‰ä¸¾ã€‚è®©æˆ‘ä»¬åœ¨é›†ç¾¤ä¸Šå®‰è£…åº”ç”¨ç¨‹åº

```sh
# Setup required permissions for creating/getting Lease objects
$ kubectl apply -f rbac.yaml
serviceaccount/leaderelection-sa created
role.rbac.authorization.k8s.io/leaderelection-role created
rolebinding.rbac.authorization.k8s.io/leaderelection-rolebinding created
# Create deployment
$ kubectl apply -f deploy.yaml
deployment.apps/leaderelection created
```
è¿™åº”è¯¥åˆ›å»ºä¸€ä¸ªåŒ…å« 3 ä¸ª Podï¼ˆå‰¯æœ¬ï¼‰çš„éƒ¨ç½²ã€‚å¦‚æœç­‰å¾…å‡ ç§’é’Ÿï¼Œåº”ä¼šçœ‹åˆ°å®ƒä»¬å¤„äº Running çŠ¶æ€ã€‚

```sh
â¯ kubectl get pods
NAME                              READY   STATUS    RESTARTS   AGE
leaderelection-6d5b456c9d-cfd2l   1/1     Running   0          19s
leaderelection-6d5b456c9d-n2kx2   1/1     Running   0          19s
leaderelection-6d5b456c9d-ph8nj   1/1     Running   0          19s
```
è¿è¡Œ Pod åï¼Œè®©æˆ‘ä»¬å°è¯•æŸ¥çœ‹å®ƒä»¬åœ¨é¢†å¯¼è€…é€‰ä¸¾è¿‡ç¨‹ä¸­åˆ›å»ºçš„ Lease lock å¯¹è±¡ã€‚

```go
$ kubectl describe lease my-lease
Name:         my-lease
Namespace:    default
Labels:       <none>
Annotations:  <none>
API Version:  coordination.k8s.io/v1
Kind:         Lease
Metadata:
...
Spec:
  Acquire Time:            2021-10-23T06:51:50.605570Z
  Holder Identity:         leaderelection-56457b6c5c-fn725
  Lease Duration Seconds:  15
  Lease Transitions:       0
  Renew Time:              2021-10-23T06:52:45.309478Z
  
 ```
 æ®æ­¤ï¼Œæˆ‘ä»¬ç›®å‰çš„é¢†å¯¼è€… pod æ˜¯ leaderelection-56457bc5c-fn725 ã€‚è®©æˆ‘ä»¬é€šè¿‡æŸ¥çœ‹æˆ‘ä»¬çš„ pod æ—¥å¿—æ¥éªŒè¯è¿™ä¸€ç‚¹ã€‚

 ```sh
 # leader pod
$ kubectl logs leaderelection-56457b6c5c-fn725
I1023 06:51:50.605439       1 leaderelection.go:248] attempting to acquire leader lease default/my-lease...
I1023 06:51:50.630111       1 leaderelection.go:258] successfully acquired lease default/my-lease
I1023 06:51:50.630141       1 main.go:57] still the leader!
I1023 06:51:50.630245       1 main.go:36] doing stuff...
# inactive pods
$ kubectl logs leaderelection-56457b6c5c-n857k
I1023 06:51:55.400797       1 leaderelection.go:248] attempting to acquire leader lease default/my-lease...
I1023 06:51:55.412780       1 main.go:60] new leader is %sleaderelection-56457b6c5c-fn725
# inactive pod
$ kubectl logs leaderelection-56457b6c5c-s48kx
I1023 06:51:52.905451       1 leaderelection.go:248] attempting to acquire leader lease default/my-lease...
I1023 06:51:52.915618       1 main.go:60] new leader is %sleaderelection-56457b6c5c-fn725
```
å°è¯•åˆ é™¤é¢†å¯¼è€… Pod ä»¥æ¨¡æ‹Ÿå´©æºƒï¼Œå¹¶æ£€æŸ¥ Lease å¯¹è±¡æ˜¯å¦é€‰å‡ºäº†æ–°çš„é¢†å¯¼è€…;)

### ä»£ç åˆ†æ

è¿™é‡Œçš„åŸºæœ¬æ€æƒ³æ˜¯ä½¿ç”¨åˆ†å¸ƒå¼é”å®šæœºåˆ¶æ¥å†³å®šå“ªä¸ªè¿›ç¨‹æˆä¸ºé¢†å¯¼è€…ã€‚è·å–é”çš„è¿‡ç¨‹å¯ä»¥æ‰§è¡Œæ‰€éœ€çš„ä»»åŠ¡ã€‚ main å‡½æ•°æ˜¯æˆ‘ä»¬åº”ç”¨ç¨‹åºçš„å…¥å£ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå¯¹é”å®šå¯¹è±¡çš„å¼•ç”¨ï¼Œå¹¶å¯åŠ¨ä¸€ä¸ªé¢†å¯¼è€…é€‰ä¸¾å¾ªç¯ã€‚

```go
func main() {
	var (
		leaseLockName      string
		leaseLockNamespace string
		podName            = os.Getenv("POD_NAME")
	)
	flag.StringVar(&leaseLockName, "lease-name", "", "Name of lease lock")
	flag.StringVar(&leaseLockNamespace, "lease-namespace", "default", "Name of lease lock namespace")
	flag.Parse()

	if leaseLockName == "" {
		klog.Fatal("missing lease-name flag")
	}
	if leaseLockNamespace == "" {
		klog.Fatal("missing lease-namespace flag")
	}

	config, err := rest.InClusterConfig()
	client = clientset.NewForConfigOrDie(config)

	if err != nil {
		klog.Fatalf("failed to get kubeconfig")
	}

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	lock := getNewLock(leaseLockName, podName, leaseLockNamespace)
	runLeaderElection(lock, ctx, podName)
}
```

æˆ‘ä»¬é¦–å…ˆè§£æ lease-name å’Œ lease-namespace æ ‡å¿—ï¼Œä»¥è·å–å‰¯æœ¬å¿…é¡»ä½¿ç”¨çš„é”å®šå¯¹è±¡çš„åç§°å’Œå‘½åç©ºé—´ã€‚ POD_NAME ç¯å¢ƒå˜é‡çš„å€¼ï¼ˆå¡«å……åœ¨ deploy.yaml æ¸…å•ä¸­ï¼‰å°†ç”¨äºæ ‡è¯† Lease å¯¹è±¡ä¸­çš„é¢†å¯¼è€…ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™äº›å‚æ•°åˆ›å»ºä¸€ä¸ªé”å®šå¯¹è±¡æ¥å¯åŠ¨é¢†å¯¼è€…é€‰ä¸¾è¿‡ç¨‹

runLeaderElection å‡½æ•°æ˜¯æˆ‘ä»¬é€šè¿‡è°ƒç”¨ RunOrDie æ¥å¯åŠ¨é¢†å¯¼è€…é€‰ä¸¾å¾ªç¯çš„åœ°æ–¹ã€‚æˆ‘ä»¬ç»™å®ƒä¼ é€’ä¸€ä¸ª LeaderElectionConfig ï¼š

```go
func runLeaderElection(lock *resourcelock.LeaseLock, ctx context.Context, id string) {
	leaderelection.RunOrDie(ctx, leaderelection.LeaderElectionConfig{
		Lock:            lock,
		ReleaseOnCancel: true,
		LeaseDuration:   15 * time.Second,
		RenewDeadline:   10 * time.Second,
		RetryPeriod:     2 * time.Second,
		Callbacks: leaderelection.LeaderCallbacks{
			OnStartedLeading: func(c context.Context) {
				doStuff()
			},
			OnStoppedLeading: func() {
				klog.Info("no longer the leader, staying inactive.")
			},
			OnNewLeader: func(current_id string) {
				if current_id == id {
					klog.Info("still the leader!")
					return
				}
				klog.Info("new leader is %s", current_id)
			},
		},
	})
}
```

ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ RunOrDie åœ¨ client-go ä¸­çš„å®ç°ã€‚

```go
// RunOrDie starts a client with the provided config or panics if the config
// fails to validate. RunOrDie blocks until leader election loop is
// stopped by ctx or it has stopped holding the leader lease
func RunOrDie(ctx context.Context, lec LeaderElectionConfig) {
	le, err := NewLeaderElector(lec)
	if err != nil {
		panic(err)
	}
	if lec.WatchDog != nil {
		lec.WatchDog.SetLeaderElection(le)
	}
	le.Run(ctx)
}
```

å®ƒä½¿ç”¨æˆ‘ä»¬ä¼ é€’ç»™å®ƒçš„ LeaderElectorConfig åˆ›å»ºä¸€ä¸ª *LeaderElector ï¼Œå¹¶åœ¨å…¶ä¸Šè°ƒç”¨ Run æ–¹æ³•ï¼š

```go
// Run starts the leader election loop. Run will not return
// before leader election loop is stopped by ctx or it has
// stopped holding the leader lease
func (le *LeaderElector) Run(ctx context.Context) {
	defer runtime.HandleCrash()
	defer func() {
		le.config.Callbacks.OnStoppedLeading()
	}()

	if !le.acquire(ctx) {
		return // ctx signalled done
	}
	ctx, cancel := context.WithCancel(ctx)
	defer cancel()
	go le.config.Callbacks.OnStartedLeading(ctx)
	le.renew(ctx)
}
```

æ­¤æ–¹æ³•è´Ÿè´£è¿è¡Œé¢†å¯¼è€…é€‰ä¸¾å¾ªç¯ã€‚å®ƒé¦–å…ˆå°è¯•è·å–é”ï¼ˆä½¿ç”¨ le.acquire ï¼‰ã€‚æˆåŠŸåï¼Œå®ƒä¼šè¿è¡Œæˆ‘ä»¬ä¹‹å‰é…ç½®çš„ OnStartedLeading å›è°ƒï¼Œå¹¶å®šæœŸç»­è®¢ç§Ÿçº¦ã€‚å¦‚æœæ— æ³•è·å–é”ï¼Œå®ƒåªéœ€è¿è¡Œ OnStoppedLeading å›è°ƒå¹¶è¿”å›

acquire å’Œ renew æ–¹æ³•å®ç°ä¸­æœ€é‡è¦çš„éƒ¨åˆ†æ˜¯å¯¹ tryAcquireOrRenew çš„è°ƒç”¨ï¼Œå®ƒåŒ…å«é”å®šæœºåˆ¶çš„æ ¸å¿ƒé€»è¾‘ã€‚

### ä¹è§‚é”å®šï¼ˆå¹¶å‘æ§åˆ¶ï¼‰

é¢†å¯¼è€…é€‰ä¸¾è¿‡ç¨‹åˆ©ç”¨ Kubernetes æ“ä½œçš„åŸå­æ€§è´¨æ¥ç¡®ä¿æ²¡æœ‰ä¸¤ä¸ªå‰¯æœ¬å¯ä»¥åŒæ—¶è·å– Lease ï¼ˆå¦åˆ™å¯èƒ½ä¼šå¯¼è‡´ç«äº‰æ¡ä»¶å’Œå…¶ä»–æ„å¤–è¡Œä¸ºï¼æ¯å½“ Lease æ›´æ–°ï¼ˆæ›´æ–°æˆ–è·å–ï¼‰æ—¶ï¼Œå…¶ä¸Šçš„ resourceVersion å­—æ®µä¹Ÿä¼šç”± Kubernetes æ›´æ–°ã€‚å½“å¦ä¸€ä¸ªè¿›ç¨‹å°è¯•åŒæ—¶æ›´æ–° Lease æ—¶ï¼ŒKubernetes ä¼šæ£€æŸ¥æ›´æ–°å¯¹è±¡çš„ resourceVersion å­—æ®µæ˜¯å¦ä¸å½“å‰å¯¹è±¡åŒ¹é… â€” å¦‚æœæ²¡æœ‰ï¼Œæ›´æ–°å°†å¤±è´¥ï¼Œä»è€Œé˜²æ­¢å¹¶å‘é—®é¢˜ï¼

### æ€»ç»“

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†é¢†å¯¼è€…é€‰ä¸¾çš„æ¦‚å¿µï¼Œä»¥åŠä¸ºä»€ä¹ˆå®ƒå¯¹äºåˆ†å¸ƒå¼ç³»ç»Ÿçš„é«˜å¯ç”¨æ€§è‡³å…³é‡è¦ã€‚æˆ‘ä»¬çœ‹äº†ä¸€ä¸‹å¦‚ä½•ä½¿ç”¨ Lease é”åœ¨ Kubernetes ä¸­å®ç°è¿™ä¸€ç‚¹ï¼Œå¹¶å°è¯•ä½¿ç”¨ kubernetes/client-go åº“è‡ªè¡Œå®ç°å®ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯•å›¾äº†è§£ Kubernetes å¦‚ä½•ä½¿ç”¨åŸå­æ“ä½œå’Œä¹è§‚é”å®šæ–¹æ³•æ¥é˜²æ­¢ç”±å¹¶å‘å¼•èµ·çš„é—®é¢˜ã€‚