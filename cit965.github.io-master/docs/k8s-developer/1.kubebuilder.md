---
sidebar_label: 1.kubebuilder
sidebar_position: 1
title: 1.kubebuilder
---


## æ¦‚å¿µ

### crdã€controllerã€operator

- crd æ˜¯ k8s ä¸ºæé«˜å¯æ‰©å±•æ€§ï¼Œè®©å¼€å‘è€…å»è‡ªå®šä¹‰èµ„æºçš„è§„èŒƒã€‚
- controller å¯ä»¥å»ç›‘å¬ crd çš„å˜åŒ–äº‹ä»¶æ¥æ·»åŠ è‡ªå®šä¹‰ä¸šåŠ¡é€»è¾‘ã€‚
- operator å°±æ˜¯ crd + controller ï¼Œæˆ‘ä»¬å¸¸è¯´çš„ k8s äºŒæ¬¡å¼€å‘æŒ‡çš„å°±æ˜¯ operator å¼€å‘ã€‚

æ€»çš„æ¥è¯´ï¼ŒCRDã€Controller å’Œ Operator åœ¨ Kubernetes ç”Ÿæ€ç³»ç»Ÿä¸­æ˜¯ç´§å¯†ç›¸å…³çš„ã€‚CRD æä¾›äº†æ‰©å±• Kubernetes API çš„èƒ½åŠ›ï¼ŒController æä¾›äº†é€šç”¨çš„èµ„æºç®¡ç†æœºåˆ¶ï¼Œè€Œ Operator åˆ™ç»“åˆè¿™ä¸¤è€…ï¼Œä¸ºå¤æ‚åº”ç”¨ç¨‹åºæä¾›é«˜çº§çš„ç®¡ç†åŠŸèƒ½ã€‚

### k8s äºŒæ¬¡å¼€å‘

Kubernetes æ˜¯ä¸€ä¸ªå¼€æºçš„å®¹å™¨ç¼–æ’ç³»ç»Ÿï¼Œç”¨äºè‡ªåŠ¨åŒ–åº”ç”¨ç¨‹åºçš„éƒ¨ç½²ã€æ‰©å±•å’Œç®¡ç†ã€‚å› å…¶åŠŸèƒ½å¼ºå¤§å’Œå¹¿æ³›çš„ç”Ÿæ€ç³»ç»Ÿæ”¯æŒï¼Œè®¸å¤šå…¬å¸éƒ½åœ¨ä½¿ç”¨ Kubernetes è¿›è¡Œåº”ç”¨éƒ¨ç½²ã€‚å¦‚æœä½ å¸Œæœ›å¯¹ Kubernetes è¿›è¡ŒäºŒæ¬¡å¼€å‘ä»¥é€‚åº”ç‰¹å®šçš„éœ€æ±‚ï¼Œæœ‰å‡ ç§å¸¸è§çš„æ–¹å¼å¯ä»¥è€ƒè™‘ï¼š

- ä½¿ç”¨ KubeBuilder æˆ– Operator SDK å¼€å‘operator: è¿™äº›å·¥å…·å¸®åŠ©ç®€åŒ–å¼€å‘ CRDs å’Œè‡ªå®šä¹‰æ§åˆ¶å™¨çš„è¿‡ç¨‹ã€‚

- å¼€å‘ç½‘ç»œæ’ä»¶ (CNI): å¦‚æœä½ éœ€è¦è‡ªå®šä¹‰ Kubernetes é›†ç¾¤çš„ç½‘ç»œè¡Œä¸ºï¼Œå¯ä»¥è€ƒè™‘å¼€å‘ä¸€ä¸ª CNI æ’ä»¶ã€‚

- å¼€å‘å­˜å‚¨æ’ä»¶ (CSI): å¦‚æœä½ éœ€è¦æ”¯æŒç‰¹å®šçš„å­˜å‚¨åç«¯æˆ–ç‰¹æ€§ï¼Œå¯ä»¥é€šè¿‡å¼€å‘ CSI æ’ä»¶æ¥æ‰©å±• Kubernetes çš„å­˜å‚¨èƒ½åŠ›ã€‚

- å¼€å‘è°ƒåº¦å™¨: å¦‚æœé»˜è®¤çš„ Kubernetes è°ƒåº¦å™¨ä¸ç¬¦åˆä½ çš„éœ€æ±‚ï¼Œä½ å¯ä»¥å¼€å‘è‡ªå·±çš„è°ƒåº¦å™¨æ¥æ›¿ä»£æˆ–å¢å¼ºé»˜è®¤çš„è°ƒåº¦é€»è¾‘ã€‚

- å¼€å‘è®¤è¯å’Œæˆæƒæ’ä»¶: é€šè¿‡å¼€å‘è‡ªå®šä¹‰çš„è®¤è¯å’Œæˆæƒæ’ä»¶ï¼Œä½ å¯ä»¥å¢å¼º Kubernetes é›†ç¾¤çš„å®‰å…¨æ€§ã€‚

- API æœåŠ¡æ‰©å±• (Aggregated API Servers): é€šè¿‡å¼€å‘èšåˆ API æœåŠ¡å™¨ï¼Œä½ å¯ä»¥å°†æ–°çš„ API æ·»åŠ åˆ° Kubernetes API æœåŠ¡å™¨ï¼Œè€Œæ— éœ€ä¿®æ”¹æ ¸å¿ƒ APIã€‚

å…¶ä¸­æœ€æ™®éçš„äºŒæ¬¡å¼€å‘å°±æ˜¯ operator å¼€å‘ï¼ŒOperator å¼€å‘ SDK æœ‰ 2 ä¸ªé€‰æ‹©ï¼š

- **kubebuilder**
- operator sdk

æ³¨æ„ï¼šåœ¨æœ¬è´¨ä¸Šå…¶å®éƒ½æ˜¯åœ¨ K8s æ§åˆ¶å™¨è¿è¡Œæ—¶ä¸Šçš„å°è£…ï¼Œä¸»è¦éƒ½æ˜¯è„šæ‰‹æ¶çš„ç”Ÿæˆï¼Œä½¿ç”¨ä½“éªŒç›¸å·®ä¸å¤§ã€‚ä½†æ˜¯æœ‰æ„æ€çš„æ˜¯ï¼ŒKubebuilder çš„ç»´æŠ¤æ–¹æ˜¯ï¼škubernetes-sigsï¼Œæ‰€ä»¥æ›´å—äººå…³æ³¨ï¼Œ**æœ¬æ–‡ä¸»è¦è®²è§£å¦‚ä½•ä½¿ç”¨ kubebuilder æ¥å¼€å‘ operator**ã€‚ä¸‹é¢æ˜¯ kubebuilder æ¶æ„å›¾ï¼š

![](http://sm.nsddd.top/sm202304081027380.png)

## å¿«é€Ÿå¼€å§‹

### å®‰è£… kubebuilder

```shell
os=$(go env GOOS)
arch=$(go env GOARCH)

# download kubebuilder and extract it to tmp
curl -L https://go.kubebuilder.io/dl/2.3.1/${os}/${arch} | tar -xz -C /tmp/

# move to a long-term location and put it on your path
# (you'll need to set the KUBEBUILDER_ASSETS env var if you put it somewhere else)
sudo mv /tmp/kubebuilder_2.3.1_${os}_${arch} /usr/local/kubebuilder
export PATH=$PATH:/usr/local/kubebuilder/bin
```

### åˆ›å»ºé¡¹ç›®

```shell
mkdir example
cd example
kubebuilder init --domain my.domain
go mod init xxx
go mod tidy
```

kubebuilder ä¼šå¸®æˆ‘ä»¬ç”Ÿæˆä¸€ä¸ªæ¡†æ¶ï¼Œæˆ‘ä»¬åœ¨æ­¤æ¡†æ¶ä¸Šè¿›è¡Œ k8s äºŒæ¬¡å¼€å‘,ä¸‹é¢æ˜¯æ¡†æ¶çš„ç›®å½•ç»“æ„ï¼š

```shell
â¯ tree -L 3
.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Makefile
â”œâ”€â”€ PROJECT
â”œâ”€â”€ README.md
â”œâ”€â”€ cmd
â”‚   â””â”€â”€ main.go
â”œâ”€â”€ config
â”‚   â”œâ”€â”€ default
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”‚   â”œâ”€â”€ manager_auth_proxy_patch.yaml
â”‚   â”‚   â””â”€â”€ manager_config_patch.yaml
â”‚   â”œâ”€â”€ manager
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”‚   â””â”€â”€ manager.yaml
â”‚   â”œâ”€â”€ prometheus
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”‚   â””â”€â”€ monitor.yaml
â”‚   â””â”€â”€ rbac
â”‚       â”œâ”€â”€ auth_proxy_client_clusterrole.yaml
â”‚       â”œâ”€â”€ auth_proxy_role.yaml
â”‚       â”œâ”€â”€ auth_proxy_role_binding.yaml
â”‚       â”œâ”€â”€ auth_proxy_service.yaml
â”‚       â”œâ”€â”€ kustomization.yaml
â”‚       â”œâ”€â”€ leader_election_role.yaml
â”‚       â”œâ”€â”€ leader_election_role_binding.yaml
â”‚       â”œâ”€â”€ role_binding.yaml
â”‚       â””â”€â”€ service_account.yaml
â”œâ”€â”€ go.mod
â”œâ”€â”€ go.sum
â””â”€â”€ hack
    â””â”€â”€ boilerplate.go.txt
```

- `Dockerfile`: ç”¨äºæ„å»º Docker é•œåƒçš„æ–‡ä»¶ã€‚
- `Makefile`: ä¸€ä¸ª Makefileï¼Œå…¶ä¸­åŒ…å«äº†ç”¨äºæ„å»ºå’Œå‘å¸ƒ Operator çš„å¸¸ç”¨å‘½ä»¤ã€‚
- `PROJECT`: é¡¹ç›®åç§°ï¼Œä»¥åŠé¡¹ç›®ä¿¡æ¯ï¼Œè¿™é‡Œæ˜¯ä¸€äº› metadata ã€‚
- `README.md`: é¡¹ç›®çš„è¯´æ˜æ–‡æ¡£ã€‚
- `cmd/`: åŒ…å«äº† Operator çš„å…¥å£ç¨‹åº `main.go`ã€‚
- `config/`: åŒ…å«äº† Operator çš„é…ç½®æ–‡ä»¶ï¼ŒåŒ…æ‹¬ RBAC æƒé™ç›¸å…³çš„ YAML æ–‡ä»¶ã€Prometheus ç›‘æ§æœåŠ¡å‘ç°ï¼ˆServiceMonitorï¼‰ç›¸å…³çš„ Yaml æ–‡ä»¶ã€æ§åˆ¶å™¨ï¼ˆManagerï¼‰éƒ¨åˆ†éƒ¨ç½²çš„ Yaml æ–‡ä»¶ã€‚
    - `default/`: åŒ…å«äº†é»˜è®¤çš„é…ç½®æ–‡ä»¶ã€‚
        - `kustomization.yaml`: Kustomize é…ç½®æ–‡ä»¶ï¼ŒæŒ‡å®šäº†éœ€è¦åº”ç”¨çš„ k8s èµ„æºç±»å‹å’Œåç§°ã€‚
        - `manager_auth_proxy_patch.yaml`: åœ¨ manager å®¹å™¨ä¸­æ·»åŠ äº† auth-proxy å®¹å™¨çš„ç›¸å…³ä¿¡æ¯ã€‚
        - `manager_config_patch.yaml`: åœ¨ manager å®¹å™¨ä¸­æ·»åŠ äº†ä¸ Operator ç›¸å…³çš„é…ç½®ä¿¡æ¯ã€‚
    - `manager/`: åŒ…å«äº†éƒ¨ç½² Operator æ‰€éœ€çš„ k8s èµ„æºæ–‡ä»¶ã€‚
        - `kustomization.yaml`: Kustomize é…ç½®æ–‡ä»¶ï¼ŒæŒ‡å®šäº†éœ€è¦åº”ç”¨çš„ k8s èµ„æºç±»å‹å’Œåç§°ã€‚
        - `manager.yaml`: éƒ¨ç½² Operator æ‰€éœ€çš„ k8s èµ„æºæ–‡ä»¶ã€‚
    - `prometheus/`: åŒ…å«äº† Prometheus ç›‘æ§ Operator æ‰€éœ€çš„ k8s èµ„æºæ–‡ä»¶ã€‚
        - `kustomization.yaml`: Kustomize é…ç½®æ–‡ä»¶ï¼ŒæŒ‡å®šäº†éœ€è¦åº”ç”¨çš„ k8s èµ„æºç±»å‹å’Œåç§°ã€‚
        - `monitor.yaml`: éƒ¨ç½² Prometheus ç›‘æ§ Operator æ‰€éœ€çš„ k8s èµ„æºæ–‡ä»¶ã€‚
    - `rbac/`: åŒ…å«äº† Operator æ‰€éœ€çš„ RBAC èµ„æºæ–‡ä»¶ã€‚
        - `auth_proxy_client_clusterrole.yaml`: é…ç½®äº†ä¸å®¢æˆ·ç«¯æˆæƒç›¸å…³çš„ ClusterRoleã€‚
        - `auth_proxy_role.yaml`: é…ç½®äº†ä¸ auth-proxy ç›¸å…³çš„ Roleã€‚
        - `auth_proxy_role_binding.yaml`: é…ç½®äº†ä¸ auth-proxy ç›¸å…³çš„ RoleBindingã€‚
        - `auth_proxy_service.yaml`: é…ç½®äº†ä¸ auth-proxy ç›¸å…³çš„ Serviceã€‚
        - `kustomization.yaml`: Kustomize é…ç½®æ–‡ä»¶ï¼ŒæŒ‡å®šäº†éœ€è¦åº”ç”¨çš„ k8s èµ„æºç±»å‹å’Œåç§°ã€‚
        - `leader_election_role.yaml`: é…ç½®äº†ä¸ leader election ç›¸å…³çš„ Roleã€‚
        - `leader_election_role_binding.yaml`: é…ç½®äº†ä¸ leader election ç›¸å…³çš„ RoleBindingã€‚
        - `role_binding.yaml`: é…ç½®äº†ä¸ Operator ç›¸å…³çš„ RoleBindingã€‚
        - `service_account.yaml`: é…ç½®äº†ä¸ Operator ç›¸å…³çš„ ServiceAccountã€‚
- `go.mod`: Go é¡¹ç›®çš„æ¨¡å—æ–‡ä»¶ã€‚
- `go.sum`: Go é¡¹ç›®çš„æ¨¡å—ä¾èµ–æ–‡ä»¶ã€‚
- `hack/`: åŒ…å«äº†ç”Ÿæˆä»£ç å’Œæ–‡æ¡£ç­‰ç›¸å…³çš„è„šæœ¬å’Œæ–‡ä»¶ã€‚
    - `boilerplate.go.txt`: ç”¨äºç”Ÿæˆ Go é¡¹ç›®æ–‡ä»¶çš„ä»£ç æ¨¡æ¿ã€‚

### åˆ›å»º API

Kubernetes çš„èµ„æºæœ¬è´¨å°±æ˜¯ä¸€ä¸ª API å¯¹è±¡ï¼Œä¸è¿‡è¿™ä¸ªå¯¹è±¡çš„ æœŸæœ›çŠ¶æ€ è¢« API Service ä¿å­˜åœ¨äº† ETCD ä¸­ï¼ˆæˆ–è€…æ˜¯å¯¹äº k3s æ¥è¯´å¯ä»¥ä¿å­˜åœ¨å…¶ä»–çš„æœ‰çŠ¶æ€æ•°æ®åº“ï¼ŒåŒ…æ‹¬ sqliteã€dqliteã€mysqlâ€¦ï¼Œç„¶åæä¾› RESTful æ¥å£ç”¨äº æ›´æ–°è¿™äº›å¯¹è±¡ã€‚

è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„ APIï¼ˆç»„/ç‰ˆæœ¬ï¼‰ä½œä¸º webapp/v1 ï¼Œå¹¶åœ¨å…¶ä¸Šåˆ›å»ºæ–°çš„ Kindï¼ˆCRDï¼‰ Guestbook ã€‚åˆ›å»ºæ—¶ä¼šå¼¹å‡ºæç¤ºé—®æˆ‘ä»¬æ˜¯å¦è¦ç”Ÿæˆ controller ä»£ç ï¼Œä¸€èˆ¬é€‰æ‹© yesã€‚

```shell
kubebuilder create api --group webapp --version v1 --kind Guestbook
```

æ–°å¢ç›®å½•ï¼š

+ `api`ï¼šåŒ…å«åˆšåˆšæ·»åŠ çš„ APIï¼Œåé¢ä¼šç»å¸¸ç¼–è¾‘è¿™é‡Œçš„ `guestbook_types.go` æ–‡ä»¶ã€‚è¿™ä¸ªæ–‡ä»¶æ˜¯ CRD ä»£ç çš„ä¸»è¦å®šä¹‰æ–‡ä»¶ã€‚
+ `config/crd`ï¼šå­˜æ”¾çš„æ˜¯ crd éƒ¨ç½²ç›¸å…³çš„ kustomize æ–‡ä»¶ã€‚
+ `config/rbac/`ï¼šåˆ†åˆ«æ˜¯ç¼–è¾‘æƒé™å’ŒæŸ¥è¯¢æƒé™çš„ `ClusterRole`
+ `samples`ï¼šå¾ˆå¥½ç†è§£ï¼ŒCR ç¤ºä¾‹æ–‡ä»¶
+ `internal` ï¼šå¾ˆå¥½ç†è§£ï¼Œå†…éƒ¨æ ¸å¿ƒä»£ç ï¼Œæˆ‘ä»¬æ‰“å¼€çœ‹çœ‹ `controllers`


æˆ‘ä»¬åœ¨ä¸Šé¢è®²è¿‡ï¼ŒCRD çš„ä»£ç å®šä¹‰ä¸»è¦åœ¨ `api/` ç›®å½•ä¸‹é¢ï¼Œæˆ‘ä»¬çœ‹ä¸€ä¸‹ä»£ç ç»“æ„ï¼š

```bash
â¯ tree api
api
â””â”€â”€ v1
    â”œâ”€â”€ groupversion_info.go
    â”œâ”€â”€ guestbook_types.go
    â””â”€â”€ zz_generated.deepcopy.go
```

`guestbook_types.go` æ–‡ä»¶ä¸»è¦çš„å®šä¹‰ï¼Œæˆ‘ä»¬çœ‹ä¸‹ spec ç»“æ„ã€‚

:::tip  details Spec ç»“å°¾çš„ç»“æ„ä½“å«ä¹‰
åœ¨Goè¯­è¨€ä¸­ï¼Œç»“æ„ä½“ä»¥ spec ç»“å°¾è¡¨ç¤ºè¯¥ç»“æ„ä½“æ˜¯ç”¨äºç‰¹å®šç›®çš„çš„è§„èŒƒç»“æ„ä½“ã€‚è¿™ç§å‘½åçº¦å®šé€šå¸¸ç”¨äºæè¿°ä¸€ä¸ªç»“æ„ä½“çš„ç”¨é€”å’ŒåŠŸèƒ½ï¼Œä»¥ä¾¿å¼€å‘äººå‘˜æ›´å¥½åœ°ç†è§£å’Œä½¿ç”¨å®ƒã€‚ä¾‹å¦‚ï¼Œ`GuestbookSpec`å®šä¹‰äº†æ‰€éœ€çš„ `Guestbook` çŠ¶æ€
:::

```go
// GuestbookSpec defines the desired state of Guestbook
type GuestbookSpec struct {
        // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster
        // Important: Run "make" to regenerate code after modifying this file

        // Foo is an example field of Guestbook. Edit guestbook_types.go to remove/update
        Foo string `json:"foo,omitempty"`
}
```

ä¸Šé¢çš„æ³¨é‡Šå†™çš„å¾ˆæ¸…æ¥šï¼ŒFoo æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥åˆ é™¤æ‰ï¼Œç„¶åæ·»åŠ è‡ªå·±éœ€è¦çš„å­—æ®µã€‚ä¿®æ”¹è¿™ä¸ªæ–‡ä»¶ååˆ©ç”¨ Makefile æŒ‡ä»¤é‡æ–°ç”Ÿæˆä»£ç ï¼Œç®€å•çš„ä¸€ä¸ªæ¡ˆä¾‹å¦‚ä¸‹ğŸ’¡ï¼š

```go
import (
	corev1 "k8s.io/api/core/v1"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

type GuestbookSpec struct {
	Replicas int32					`json:"replicas,omitempty"`
    Template corev1.PodTemplateSpec	`json:"template,omitempty"`
}
```


### åœ¨é›†ç¾¤è¿è¡Œ

ç”±äºæˆ‘ä»¬æ˜¯åŸºäºk8såšäºŒæ¬¡å¼€å‘ï¼Œè¦æ±‚åœ¨ .kube ç›®å½•ä¸‹æœ‰å¯ç”¨é›†ç¾¤çš„ kubeconfig é…ç½®æ–‡ä»¶ã€‚


1. å°† CRD å®‰è£…åˆ°é›†ç¾¤ä¸­ï¼š

```shell
make install
```

2. æœ¬åœ°è¿è¡Œæ‚¨çš„æ§åˆ¶å™¨

```shell
make run
```

### å¸è½½ crds

```shell
make uninstall
```

## å†™ä¸€ä¸ª cronJob

è®©æˆ‘ä»¬å‡è£…åŒå€¦äº† Kubernetes ä¸­ CronJob æ§åˆ¶å™¨çš„é Kubebuilder å®ç°çš„ç»´æŠ¤è´Ÿæ‹…ï¼Œæˆ‘ä»¬æƒ³ä½¿ç”¨ KubeBuilder é‡å†™å®ƒã€‚

CronJob æ§åˆ¶å™¨çš„å®šæœŸåœ¨ Kubernetes é›†ç¾¤ä¸Šè¿è¡Œä¸€æ¬¡æ€§ä»»åŠ¡ã€‚å®ƒé€šè¿‡åœ¨jobæ§åˆ¶å™¨ä¹‹ä¸Šæ„å»ºæ¥å®ç°è¿™ä¸€ç‚¹ï¼Œjobæ§åˆ¶å™¨çš„ä»»åŠ¡æ˜¯è¿è¡Œä¸€æ¬¡æ€§ä»»åŠ¡ä¸€æ¬¡ï¼Œçœ‹åˆ°å®ƒä»¬å®Œæˆã€‚

### æ­å»ºé¡¹ç›®

å¦‚å¿«é€Ÿå¼€å§‹ä¸­æ‰€è¿°ï¼Œæˆ‘ä»¬éœ€è¦æ­å»ºä¸€ä¸ªæ–°é¡¹ç›®çš„è„šæ‰‹æ¶ã€‚ç¡®ä¿ä½ å·²ç»å®‰è£…äº† Kubebuilderï¼Œç„¶åæ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

```shell
# we'll use a domain of tutorial.kubebuilder.io,
# so all API groups will be <group>.tutorial.kubebuilder.io.
kubebuilder init --domain tutorial.kubebuilder.io
```

æˆ‘ä»¬å…ˆæ¥çœ‹ä¸‹å¯åŠ¨é…ç½®ç›®å½• *config/* ï¼Œç°åœ¨å®ƒåªåŒ…å«ç¾¤é›†ä¸Šå¯åŠ¨æ§åˆ¶å™¨æ‰€éœ€çš„ Kustomize YAML å®šä¹‰ï¼Œä½†æ˜¯ä¸€æ—¦æˆ‘ä»¬å¼€å§‹ç¼–å†™æ§åˆ¶å™¨ï¼Œå®ƒè¿˜å°†åŒ…å«æˆ‘ä»¬çš„ crd èµ„æºå®šä¹‰ã€RBAC é…ç½®å’Œ Webhooké…ç½®ã€‚


Kubebuilder æ­å»ºäº†æˆ‘ä»¬é¡¹ç›®çš„åŸºæœ¬å…¥å£ç‚¹ï¼š main.go ã€‚ æ¥ä¸‹æ¥è®©æˆ‘ä»¬æ¥çœ‹çœ‹ main.go

### main.go

æ¯ç»„æ§åˆ¶å™¨éƒ½éœ€è¦ä¸€ä¸ª scheme ï¼Œè¯¥æ–¹æ¡ˆæä¾›ç§ç±»ä¸å…¶ç›¸åº”çš„ Go ç±»å‹ä¹‹é—´çš„æ˜ å°„ã€‚å½“æˆ‘ä»¬ç¼–å†™ API å®šä¹‰æ—¶ï¼Œæˆ‘ä»¬å°†æ›´å¤šåœ°è®¨è®ºç§ç±»ï¼Œæ‰€ä»¥è¯·è®°ä½è¿™ä¸€ç‚¹ï¼Œä»¥ä¾¿ç¨åä½¿ç”¨ã€‚

```go
var (
    scheme   = runtime.NewScheme()
    setupLog = ctrl.Log.WithName("setup")
)

func init() {

    // +kubebuilder:scaffold:scheme
}

func main() {
    var metricsAddr string
    flag.StringVar(&metricsAddr, "metrics-addr", ":8080", "The address the metric endpoint binds to.")
    flag.Parse()

    ctrl.SetLogger(zap.New(zap.UseDevMode(true)))

    mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{Scheme: scheme, MetricsBindAddress: metricsAddr})
    if err != nil {
        setupLog.Error(err, "unable to start manager")
        os.Exit(1)
    }
```


- æˆ‘ä»¬ä¸º metrics è®¾ç½®äº†ä¸€äº›åŸºæœ¬ flagsã€‚
- æˆ‘ä»¬å®ä¾‹åŒ–ä¸€ä¸ª manager ï¼Œå®ƒè·Ÿè¸ªè¿è¡Œæˆ‘ä»¬æ‰€æœ‰çš„æ§åˆ¶å™¨ï¼Œä»¥åŠè®¾ç½® API æœåŠ¡å™¨çš„å…±äº«ç¼“å­˜å’Œå®¢æˆ·ç«¯ï¼ˆè¯·æ³¨æ„ï¼Œæˆ‘ä»¬å‘Šè¯‰manager æˆ‘ä»¬çš„ schemeï¼‰
- æˆ‘ä»¬è¿è¡Œæˆ‘ä»¬çš„ manager ï¼Œå®ƒåè¿‡æ¥è¿è¡Œæˆ‘ä»¬æ‰€æœ‰çš„æ§åˆ¶å™¨å’Œ webhookã€‚ç®¡ç†å™¨è®¾ç½®ä¸ºè¿è¡Œï¼Œç›´åˆ°æ”¶åˆ°æ­£å¸¸å…³é—­ä¿¡å·ã€‚è¿™æ ·ï¼Œå½“æˆ‘ä»¬åœ¨ Kubernetes ä¸Šè¿è¡Œæ—¶ï¼Œæˆ‘ä»¬åœ¨ä¼˜é›…çš„ pod ç»ˆæ­¢æ–¹é¢è¡¨ç°å¾—å¾ˆå¥½ã€‚

è¯·æ³¨æ„ï¼Œmanager å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼é™åˆ¶æ‰€æœ‰æ§åˆ¶å™¨å°†ç›‘è§†èµ„æºçš„å‘½åç©ºé—´ï¼š


```go
    mgr, err = ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{
        Scheme:             scheme,
        Namespace:          namespace,
        MetricsBindAddress: metricsAddr,
    })
```   
ä¸Šé¢çš„ç¤ºä¾‹ä¼šå°†é¡¹ç›®çš„èŒƒå›´æ›´æ”¹ä¸ºå•ä¸ªå‘½åç©ºé—´ã€‚åœ¨æ­¤æ–¹æ¡ˆä¸­ï¼Œè¿˜å»ºè®®é€šè¿‡å°†é»˜è®¤çš„ ClusterRole å’Œ ClusterRoleBinding åˆ†åˆ«æ›¿æ¢ä¸º Role å’Œ RoleBinding æ¥é™åˆ¶å¯¹æ­¤å‘½åç©ºé—´æä¾›çš„æˆæƒ æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æœ‰å…³ä½¿ç”¨ RBAC æˆæƒçš„ kubernetes æ–‡æ¡£ã€‚

æ­¤å¤–ï¼Œè¿˜å¯ä»¥ä½¿ç”¨ MultiNamespacedCacheBuilder æ¥ç›‘è§†ä¸€ç»„ç‰¹å®šçš„å‘½åç©ºé—´ï¼š

```go
    var namespaces []string // List of Namespaces

    mgr, err = ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{
        Scheme:             scheme,
        NewCache:           cache.MultiNamespacedCacheBuilder(namespaces),
        MetricsBindAddress: fmt.Sprintf("%s:%d", metricsHost, metricsPort),
    })

        // +kubebuilder:scaffold:builder

    setupLog.Info("starting manager")
    if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil {
        setupLog.Error(err, "problem running manager")
        os.Exit(1)
    }
}
```


### æ·»åŠ ä¸€ä¸ªæ–°çš„ api

```shell
kubebuilder create api --group batch --version v1 --kind CronJob
```

æˆ‘ä»¬ç¬¬ä¸€æ¬¡ä¸ºæ¯ä¸ªç»„ç‰ˆæœ¬è°ƒç”¨æ­¤å‘½ä»¤æ—¶ï¼Œå®ƒå°†ä¸ºæ–°çš„ç»„ç‰ˆæœ¬åˆ›å»ºä¸€ä¸ªç›®å½•ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåˆ›å»º api/v1/ ç›®å½•ï¼Œå¯¹åº”äº batch.tutorial.kubebuilder.io/v1 ï¼ˆè¿˜è®°å¾—æˆ‘ä»¬ä»ä¸€å¼€å§‹å°±è®¾ç½®çš„ --domain å—ï¼Ÿ

å®ƒè¿˜ä¸ºæˆ‘ä»¬çš„ CronJob Kindç”Ÿæˆä¸€ä¸ª api/v1/cronjob_types.go æ–‡ä»¶ã€‚æ¯æ¬¡æˆ‘ä»¬è°ƒç”¨* kubebuilder create api*å‘½ä»¤æ—¶ï¼Œå®ƒéƒ½ä¼šç”Ÿæˆä¸€ä¸ªå¯¹åº”èµ„æºçš„æ–°æ–‡ä»¶ã€‚

```go
package v1

import (
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

/*
Next, we define types for the Spec and Status of our Kind.  Kubernetes functions
by reconciling desired state (`Spec`) with actual cluster state (other objects'
`Status`) and external state, and then recording what it observed (`Status`).
Thus, every *functional* object includes spec and status.  A few types, like
`ConfigMap` don't follow this pattern, since they don't encode desired state,
but most types do.
*/
// EDIT THIS FILE!  THIS IS SCAFFOLDING FOR YOU TO OWN!
// NOTE: json tags are required.  Any new fields you add must have json tags for the fields to be serialized.

// CronJobSpec defines the desired state of CronJob
type CronJobSpec struct {
	// INSERT ADDITIONAL SPEC FIELDS - desired state of cluster
	// Important: Run "make" to regenerate code after modifying this file
}

// CronJobStatus defines the observed state of CronJob
type CronJobStatus struct {
	// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
	// Important: Run "make" to regenerate code after modifying this file
}

/*
Next, we define the types corresponding to actual Kinds, `CronJob` and `CronJobList`.
`CronJob` is our root type, and describes the `CronJob` kind.  Like all Kubernetes objects, it contains
`TypeMeta` (which describes API version and Kind), and also contains `ObjectMeta`, which holds things
like name, namespace, and labels.

`CronJobList` is simply a container for multiple `CronJob`s.  It's the Kind used in bulk operations,
like LIST.

In general, we never modify either of these -- all modifications go in either Spec or Status.

That little `+kubebuilder:object:root` comment is called a marker.  We'll see
more of them in a bit, but know that they act as extra metadata, telling
[controller-tools](https://github.com/kubernetes-sigs/controller-tools) (our code and YAML generator) extra information.
This particular one tells the `object` generator that this type represents
a Kind.  Then, the `object` generator generates an implementation of the
[runtime.Object](https://pkg.go.dev/k8s.io/apimachinery/pkg/runtime?tab=doc#Object) interface for us, which is the standard
interface that all types representing Kinds must implement.
*/

//+kubebuilder:object:root=true
//+kubebuilder:subresource:status

// CronJob is the Schema for the cronjobs API
type CronJob struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`

	Spec   CronJobSpec   `json:"spec,omitempty"`
	Status CronJobStatus `json:"status,omitempty"`
}

//+kubebuilder:object:root=true

// CronJobList contains a list of CronJob
type CronJobList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []CronJob `json:"items"`
}

/*
Finally, we add the Go types to the API group.  This allows us to add the
types in this API group to any [Scheme](https://pkg.go.dev/k8s.io/apimachinery/pkg/runtime?tab=doc#Scheme).
*/
func init() {
	SchemeBuilder.Register(&CronJob{}, &CronJobList{})
}
```
Kubernetes é€šè¿‡å°†æœŸæœ›çŠ¶æ€(Spec) ä¸å®é™…é›†ç¾¤çŠ¶æ€ï¼ˆå…¶ä»–å¯¹è±¡çš„ Statusï¼‰å’Œå¤–éƒ¨çŠ¶æ€è¿›è¡Œåè°ƒï¼Œç„¶åè®°å½•å®ƒè§‚å¯Ÿåˆ°çš„å†…å®¹ ï¼ˆ Status ï¼‰ æ¥å‘æŒ¥ä½œç”¨ã€‚å› æ­¤ï¼Œæ¯ä¸ªåŠŸèƒ½å¯¹è±¡éƒ½åŒ…æ‹¬è§„èŒƒå’ŒçŠ¶æ€ã€‚ä¸€äº›ç±»å‹ï¼ˆå¦‚ ConfigMap ï¼‰ä¸éµå¾ªæ­¤æ¨¡å¼ï¼Œå› ä¸ºå®ƒä»¬ä¸å¯¹æ‰€éœ€çŠ¶æ€è¿›è¡Œç¼–ç ï¼Œä½†å¤§å¤šæ•°ç±»å‹éƒ½è¿™æ ·åšã€‚

```go
// EDIT THIS FILE!  THIS IS SCAFFOLDING FOR YOU TO OWN!
// NOTE: json tags are required.  Any new fields you add must have json tags for the fields to be serialized.

// CronJobSpec defines the desired state of CronJob
type CronJobSpec struct {
    // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster
    // Important: Run "make" to regenerate code after modifying this file
}

// CronJobStatus defines the observed state of CronJob
type CronJobStatus struct {
    // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
    // Important: Run "make" to regenerate code after modifying this file
}
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰ CronJob å’Œ CronJobList å¯¹åº”çš„ç±»å‹ã€‚ CronJob æ˜¯æˆ‘ä»¬çš„æ ¹ç±»å‹ï¼Œæè¿°äº† CronJob ç§ã€‚åƒæ‰€æœ‰ Kubernetes å¯¹è±¡ä¸€æ ·ï¼Œå®ƒåŒ…å« TypeMeta ï¼ˆæè¿° API ç‰ˆæœ¬å’Œç§ç±»ï¼‰ï¼Œè¿˜åŒ…å« ObjectMeta ä¸ªï¼Œå…¶ä¸­åŒ…å«åç§°ã€å‘½åç©ºé—´å’Œæ ‡ç­¾ç­‰å†…å®¹ã€‚CronJobList åªæ˜¯å¤šä¸ª CronJob çš„å®¹å™¨.

```go
// +kubebuilder:object:root=true

// CronJob is the Schema for the cronjobs API
type CronJob struct {
    metav1.TypeMeta   `json:",inline"`
    metav1.ObjectMeta `json:"metadata,omitempty"`

    Spec   CronJobSpec   `json:"spec,omitempty"`
    Status CronJobStatus `json:"status,omitempty"`
}

// +kubebuilder:object:root=true

// CronJobList contains a list of CronJob
type CronJobList struct {
    metav1.TypeMeta `json:",inline"`
    metav1.ListMeta `json:"metadata,omitempty"`
    Items           []CronJob `json:"items"`
}
```

æœ€åï¼Œæˆ‘ä»¬å°† Go ç±»å‹æ·»åŠ åˆ° API ç»„ã€‚è¿™å…è®¸æˆ‘ä»¬å°†æ­¤ API ç»„ä¸­çš„ç±»å‹æ·»åŠ åˆ°ä»»ä½• scheme ä¸­ã€‚

```go
func init() {
    SchemeBuilder.Register(&CronJob{}, &CronJobList{})
}
```

### å®šä¹‰ api

åœ¨ Kubernetes ä¸­ï¼Œæˆ‘ä»¬å¯¹å¦‚ä½•è®¾è®¡ API æœ‰ä¸€äº›è§„åˆ™ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ‰€æœ‰åºåˆ—åŒ–å­—æ®µéƒ½å¿…é¡»ä¸º camelCase ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨ JSON ç»“æ„æ ‡ç­¾æ¥æŒ‡å®šè¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨ omitempty struct æ ‡ç­¾æ¥æ ‡è®°å­—æ®µåœ¨ä¸ºç©ºæ—¶åº”ä»åºåˆ—åŒ–ä¸­çœç•¥ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹æˆ‘ä»¬çš„ CronJob å¯¹è±¡æ˜¯ä»€ä¹ˆæ ·çš„ï¼

```go
/*
Copyright 2023 The Kubernetes authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/
// +kubebuilder:docs-gen:collapse=Apache License

/*
 */

package v1

/*
 */

import (
	batchv1 "k8s.io/api/batch/v1"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// EDIT THIS FILE!  THIS IS SCAFFOLDING FOR YOU TO OWN!
// NOTE: json tags are required.  Any new fields you add must have json tags for the fields to be serialized.

// +kubebuilder:docs-gen:collapse=Imports

/*
 First, let's take a look at our spec.  As we discussed before, spec holds
 *desired state*, so any "inputs" to our controller go here.

 Fundamentally a CronJob needs the following pieces:

 - A schedule (the *cron* in CronJob)
 - A template for the Job to run (the
 *job* in CronJob)

 We'll also want a few extras, which will make our users' lives easier:

 - A deadline for starting jobs (if we miss this deadline, we'll just wait till
   the next scheduled time)
 - What to do if multiple jobs would run at once (do we wait? stop the old one? run both?)
 - A way to pause the running of a CronJob, in case something's wrong with it
 - Limits on old job history

 Remember, since we never read our own status, we need to have some other way to
 keep track of whether a job has run.  We can use at least one old job to do
 this.

 We'll use several markers (`// +comment`) to specify additional metadata.  These
 will be used by [controller-tools](https://github.com/kubernetes-sigs/controller-tools) when generating our CRD manifest.
 As we'll see in a bit, controller-tools will also use GoDoc to form descriptions for
 the fields.
*/

// CronJobSpec defines the desired state of CronJob
type CronJobSpec struct {
	//+kubebuilder:validation:MinLength=0

	// The schedule in Cron format, see https://en.wikipedia.org/wiki/Cron.
	Schedule string `json:"schedule"`

	//+kubebuilder:validation:Minimum=0

	// Optional deadline in seconds for starting the job if it misses scheduled
	// time for any reason.  Missed jobs executions will be counted as failed ones.
	// +optional
	StartingDeadlineSeconds *int64 `json:"startingDeadlineSeconds,omitempty"`

	// Specifies how to treat concurrent executions of a Job.
	// Valid values are:
	// - "Allow" (default): allows CronJobs to run concurrently;
	// - "Forbid": forbids concurrent runs, skipping next run if previous run hasn't finished yet;
	// - "Replace": cancels currently running job and replaces it with a new one
	// +optional
	ConcurrencyPolicy ConcurrencyPolicy `json:"concurrencyPolicy,omitempty"`

	// This flag tells the controller to suspend subsequent executions, it does
	// not apply to already started executions.  Defaults to false.
	// +optional
	Suspend *bool `json:"suspend,omitempty"`

	// Specifies the job that will be created when executing a CronJob.
	JobTemplate batchv1.JobTemplateSpec `json:"jobTemplate"`

	//+kubebuilder:validation:Minimum=0

	// The number of successful finished jobs to retain.
	// This is a pointer to distinguish between explicit zero and not specified.
	// +optional
	SuccessfulJobsHistoryLimit *int32 `json:"successfulJobsHistoryLimit,omitempty"`

	//+kubebuilder:validation:Minimum=0

	// The number of failed finished jobs to retain.
	// This is a pointer to distinguish between explicit zero and not specified.
	// +optional
	FailedJobsHistoryLimit *int32 `json:"failedJobsHistoryLimit,omitempty"`
}

/*
 We define a custom type to hold our concurrency policy.  It's actually
 just a string under the hood, but the type gives extra documentation,
 and allows us to attach validation on the type instead of the field,
 making the validation more easily reusable.
*/

// ConcurrencyPolicy describes how the job will be handled.
// Only one of the following concurrent policies may be specified.
// If none of the following policies is specified, the default one
// is AllowConcurrent.
// +kubebuilder:validation:Enum=Allow;Forbid;Replace
type ConcurrencyPolicy string

const (
	// AllowConcurrent allows CronJobs to run concurrently.
	AllowConcurrent ConcurrencyPolicy = "Allow"

	// ForbidConcurrent forbids concurrent runs, skipping next run if previous
	// hasn't finished yet.
	ForbidConcurrent ConcurrencyPolicy = "Forbid"

	// ReplaceConcurrent cancels currently running job and replaces it with a new one.
	ReplaceConcurrent ConcurrencyPolicy = "Replace"
)

/*
 Next, let's design our status, which holds observed state.  It contains any information
 we want users or other controllers to be able to easily obtain.

 We'll keep a list of actively running jobs, as well as the last time that we successfully
 ran our job.  Notice that we use `metav1.Time` instead of `time.Time` to get the stable
 serialization, as mentioned above.
*/

// CronJobStatus defines the observed state of CronJob
type CronJobStatus struct {
	// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
	// Important: Run "make" to regenerate code after modifying this file

	// A list of pointers to currently running jobs.
	// +optional
	Active []corev1.ObjectReference `json:"active,omitempty"`

	// Information when was the last time the job was successfully scheduled.
	// +optional
	LastScheduleTime *metav1.Time `json:"lastScheduleTime,omitempty"`
}

/*
 Finally, we have the rest of the boilerplate that we've already discussed.
 As previously noted, we don't need to change this, except to mark that
 we want a status subresource, so that we behave like built-in kubernetes types.
*/

//+kubebuilder:object:root=true
//+kubebuilder:subresource:status

// CronJob is the Schema for the cronjobs API
type CronJob struct {
	/*
	 */
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`

	Spec   CronJobSpec   `json:"spec,omitempty"`
	Status CronJobStatus `json:"status,omitempty"`
}

//+kubebuilder:object:root=true

// CronJobList contains a list of CronJob
type CronJobList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []CronJob `json:"items"`
}

func init() {
	SchemeBuilder.Register(&CronJob{}, &CronJobList{})
}

//+kubebuilder:docs-gen:collapse=Root Object Definitions
```

é¦–å…ˆï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹æˆ‘ä»¬çš„ spec ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€è®¨è®ºçš„ï¼Œspec ä¿å­˜é¢„æœŸçŠ¶æ€ï¼Œå› æ­¤æ§åˆ¶å™¨çš„ä»»ä½•â€œè¾“å…¥â€éƒ½åœ¨è¿™é‡Œã€‚

ä»æ ¹æœ¬ä¸Šè¯´ï¼ŒCronJob éœ€è¦ä»¥ä¸‹éƒ¨åˆ†ï¼š
- ä¸€ä¸ª scheduleï¼ˆCronJobä¸­çš„cronï¼‰
- è¦è¿è¡Œ job çš„æ¨¡æ¿ï¼ˆCronJob ä¸­çš„jobï¼‰

æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›é¢å¤–çš„åŠŸèƒ½ï¼š
- å¼€å§‹å·¥ä½œçš„æˆªæ­¢æ—¥æœŸï¼ˆå¦‚æœæˆ‘ä»¬é”™è¿‡äº†è¿™ä¸ªæˆªæ­¢æ—¥æœŸï¼Œæˆ‘ä»¬å°†ç­‰åˆ°ä¸‹ä¸€ä¸ªé¢„å®šæ—¶é—´ï¼‰
- å¦‚æœå¤šä¸ªjobåŒæ—¶è¿è¡Œæ€ä¹ˆåŠï¼ˆæˆ‘ä»¬ç­‰å¾…å—ï¼Ÿåœæ­¢æ—§çš„ï¼ŸåŒæ—¶è¿è¡Œä¸¤ä¸ªï¼Ÿï¼‰
- ä¸€ç§æš‚åœCronJobè¿è¡Œçš„æ–¹æ³•ï¼Œä»¥é˜²å‡ºç°é—®é¢˜
- å¯¹æ—§jobå†å²è®°å½•çš„é™åˆ¶

è¯·è®°ä½ï¼Œç”±äºæˆ‘ä»¬ä»ä¸è¯»å–è‡ªå·±çš„çŠ¶æ€ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦å…¶ä»–æ–¹æ³•æ¥è·Ÿè¸ªjobæ˜¯å¦å·²è¿è¡Œã€‚æˆ‘ä»¬è‡³å°‘å¯ä»¥ä½¿ç”¨ä¸€ä¸ªæ—§ job æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚

æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰ç±»å‹æ¥ä¿å­˜æˆ‘ä»¬çš„å¹¶å‘ç­–ç•¥ã€‚å®ƒå®é™…ä¸Šåªæ˜¯ä¸€ä¸ªåº•å±‚çš„å­—ç¬¦ä¸²ï¼Œä½†ç±»å‹æä¾›äº†é¢å¤–çš„æ–‡æ¡£ï¼Œå¹¶å…è®¸æˆ‘ä»¬åœ¨ç±»å‹è€Œä¸æ˜¯å­—æ®µä¸Šé™„åŠ éªŒè¯ï¼Œä½¿éªŒè¯æ›´å®¹æ˜“é‡ç”¨ã€‚

```go
// ConcurrencyPolicy describes how the job will be handled.
// Only one of the following concurrent policies may be specified.
// If none of the following policies is specified, the default one
// is AllowConcurrent.
// +kubebuilder:validation:Enum=Allow;Forbid;Replace
type ConcurrencyPolicy string

const (
    // AllowConcurrent allows CronJobs to run concurrently.
    AllowConcurrent ConcurrencyPolicy = "Allow"

    // ForbidConcurrent forbids concurrent runs, skipping next run if previous
    // hasn't finished yet.
    ForbidConcurrent ConcurrencyPolicy = "Forbid"

    // ReplaceConcurrent cancels currently running job and replaces it with a new one.
    ReplaceConcurrent ConcurrencyPolicy = "Replace"
)
```

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬è®¾è®¡æˆ‘ä»¬çš„çŠ¶æ€ï¼Œå®ƒåŒ…å«è§‚å¯ŸçŠ¶æ€ã€‚å®ƒåŒ…å«æˆ‘ä»¬å¸Œæœ›ç”¨æˆ·æˆ–å…¶ä»–æ§åˆ¶è€…èƒ½å¤Ÿè½»æ¾è·å–çš„ä»»ä½•ä¿¡æ¯ã€‚

æˆ‘ä»¬å°†ä¿ç•™æ´»åŠ¨æ­£åœ¨è¿è¡Œçš„ jobs çš„åˆ—è¡¨ï¼Œä»¥åŠä¸Šæ¬¡æˆåŠŸè¿è¡Œ job çš„æ—¶é—´ã€‚

```go
// CronJobStatus defines the observed state of CronJob
type CronJobStatus struct {
    // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
    // Important: Run "make" to regenerate code after modifying this file

    // A list of pointers to currently running jobs.
    // +optional
    Active []corev1.ObjectReference `json:"active,omitempty"`

    // Information when was the last time the job was successfully scheduled.
    // +optional
    LastScheduleTime *metav1.Time `json:"lastScheduleTime,omitempty"`
}
```

ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ª APIï¼Œæˆ‘ä»¬éœ€è¦ç¼–å†™ä¸€ä¸ªæ§åˆ¶å™¨æ¥å®é™…å®ç°è¯¥åŠŸèƒ½ã€‚

### æ§åˆ¶å™¨

æ§åˆ¶å™¨æ˜¯ Kubernetes å’Œä»»ä½• operator çš„æ ¸å¿ƒã€‚

æ§åˆ¶å™¨çš„å·¥ä½œæ˜¯ç¡®ä¿å¯¹äºä»»ä½•ç»™å®šå¯¹è±¡ï¼Œä¸–ç•Œçš„å®é™…çŠ¶æ€ï¼ˆé›†ç¾¤çŠ¶æ€å’Œæ½œåœ¨çš„å¤–éƒ¨çŠ¶æ€ï¼Œå¦‚ä¸º Kubelet è¿è¡Œå®¹å™¨æˆ–ä¸ºäº‘æä¾›å•†è¿è¡Œè´Ÿè½½å‡è¡¡å™¨ï¼‰ä¸å¯¹è±¡ä¸­çš„æ‰€éœ€çŠ¶æ€åŒ¹é…ã€‚æ¯ä¸ªæ§åˆ¶å™¨ä¸“æ³¨äºä¸€ç§æ ¹ç§ç±»ï¼Œä½†å¯ä»¥ä¸å…¶ä»–ç§ç±»äº¤äº’ã€‚

æˆ‘ä»¬ç§°æ­¤è¿‡ç¨‹ä¸º reconcilingï¼ˆåè°ƒï¼‰ã€‚


åœ¨æ§åˆ¶å™¨è¿è¡Œæ—¶ä¸­ï¼Œå®ç°ç‰¹å®šç±»å‹çš„åè°ƒçš„é€»è¾‘ç§°ä¸ºåè°ƒå™¨ã€‚åè°ƒå™¨è·å–å¯¹è±¡çš„åç§°ï¼Œå¹¶è¿”å›æˆ‘ä»¬æ˜¯å¦éœ€è¦é‡è¯•ï¼ˆä¾‹å¦‚ï¼Œåœ¨å‡ºç°é”™è¯¯æˆ–å‘¨æœŸæ€§æ§åˆ¶å™¨çš„æƒ…å†µä¸‹ï¼Œå¦‚ HorizontalPodAutoscalerï¼‰ã€‚

```go
/*
Copyright 2022.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/
// +kubebuilder:docs-gen:collapse=Apache License

/*
First, we start out with some standard imports.
As before, we need the core controller-runtime library, as well as
the client package, and the package for our API types.
*/
package controllers

import (
	"context"

	"k8s.io/apimachinery/pkg/runtime"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"

	batchv1 "tutorial.kubebuilder.io/project/api/v1"
)

/*
Next, kubebuilder has scaffolded a basic reconciler struct for us.
Pretty much every reconciler needs to log, and needs to be able to fetch
objects, so these are added out of the box.
*/

// CronJobReconciler reconciles a CronJob object
type CronJobReconciler struct {
	client.Client
	Scheme *runtime.Scheme
}

/*
Most controllers eventually end up running on the cluster, so they need RBAC
permissions, which we specify using controller-tools [RBAC
markers](/reference/markers/rbac.md).  These are the bare minimum permissions
needed to run.  As we add more functionality, we'll need to revisit these.
*/

// +kubebuilder:rbac:groups=batch.tutorial.kubebuilder.io,resources=cronjobs,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=batch.tutorial.kubebuilder.io,resources=cronjobs/status,verbs=get;update;patch

/*
The `ClusterRole` manifest at `config/rbac/role.yaml` is generated from the above markers via controller-gen with the following command:
*/

// make manifests

/*
NOTE: If you receive an error, please run the specified command in the error and re-run `make manifests`.
*/

/*
`Reconcile` actually performs the reconciling for a single named object.
Our [Request](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/reconcile?tab=doc#Request) just has a name, but we can use the client to fetch
that object from the cache.

We return an empty result and no error, which indicates to controller-runtime that
we've successfully reconciled this object and don't need to try again until there's
some changes.

Most controllers need a logging handle and a context, so we set them up here.

The [context](https://golang.org/pkg/context/) is used to allow cancelation of
requests, and potentially things like tracing.  It's the first argument to all
client methods.  The `Background` context is just a basic context without any
extra data or timing restrictions.

The logging handle lets us log.  controller-runtime uses structured logging through a
library called [logr](https://github.com/go-logr/logr).  As we'll see shortly,
logging works by attaching key-value pairs to a static message.  We can pre-assign
some pairs at the top of our reconcile method to have those attached to all log
lines in this reconciler.
*/
func (r *CronJobReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	_ = log.FromContext(ctx)

	// your logic here

	return ctrl.Result{}, nil
}

/*
Finally, we add this reconciler to the manager, so that it gets started
when the manager is started.

For now, we just note that this reconciler operates on `CronJob`s.  Later,
we'll use this to mark that we care about related objects as well.

*/

func (r *CronJobReconciler) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		For(&batchv1.CronJob{}).
		Complete(r)
}
```

æ¥ä¸‹æ¥ï¼Œkubebuilder ä¸ºæˆ‘ä»¬æ­å»ºäº†ä¸€ä¸ªåŸºæœ¬çš„åè°ƒå™¨ç»“æ„ã€‚å‡ ä¹æ¯ä¸ªåè°ƒå™¨éƒ½éœ€è¦ log ï¼Œå¹¶ä¸”éœ€è¦èƒ½å¤Ÿè·å–å¯¹è±¡ï¼Œå› æ­¤è¿™äº›å¯¹è±¡æ˜¯å¼€ç®±å³ç”¨çš„ã€‚

```go
// CronJobReconciler reconciles a CronJob object
type CronJobReconciler struct {
    client.Client
    Log    logr.Logger
    Scheme *runtime.Scheme
}
```

å¤§å¤šæ•°æ§åˆ¶å™¨æœ€ç»ˆä¼šåœ¨ç¾¤é›†ä¸Šè¿è¡Œï¼Œå› æ­¤å®ƒä»¬éœ€è¦ RBAC æƒé™ï¼Œæˆ‘ä»¬ä½¿ç”¨æ§åˆ¶å™¨å·¥å…· RBAC æ ‡è®°æŒ‡å®šè¿™äº›æƒé™ã€‚è¿™äº›æ˜¯è¿è¡Œæ‰€éœ€çš„æœ€ä½æƒé™ã€‚

```shell
// +kubebuilder:rbac:groups=batch.tutorial.kubebuilder.io,resources=cronjobs,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=batch.tutorial.kubebuilder.io,resources=cronjobs/status,verbs=get;update;patch
```

Reconcile å®é™…ä¸Šå¯¹å•ä¸ªå‘½åå¯¹è±¡æ‰§è¡Œåè°ƒã€‚æˆ‘ä»¬çš„è¯·æ±‚åªæœ‰ä¸€ä¸ªåç§°ï¼Œä½†æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®¢æˆ·ç«¯ä»ç¼“å­˜ä¸­è·å–è¯¥å¯¹è±¡ã€‚


å¤§å¤šæ•°æ§åˆ¶å™¨éƒ½éœ€è¦ log å’Œ contextï¼Œå› æ­¤æˆ‘ä»¬åœ¨æ­¤å¤„è®¾ç½®å®ƒä»¬ã€‚context å…è®¸æˆ‘ä»¬ä¼ é€’ä¸Šä¸‹æ–‡ï¼Œlog å…è®¸æˆ‘ä»¬è®°å½•æ—¥å¿—ã€‚

```go
func (r *CronJobReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) {
    _ = context.Background()
    _ = r.Log.WithValues("cronjob", req.NamespacedName)

    // your logic here

    return ctrl.Result{}, nil
}
```

æœ€åï¼Œæˆ‘ä»¬å°†æ­¤åè°ƒå™¨æ·»åŠ åˆ°ç®¡ç†å™¨ï¼Œä»¥ä¾¿åœ¨ç®¡ç†å™¨å¯åŠ¨æ—¶å¯åŠ¨å®ƒã€‚

```go
func (r *CronJobReconciler) SetupWithManager(mgr ctrl.Manager) error {
    return ctrl.NewControllerManagedBy(mgr).
        For(&batchv1.CronJob{}).
        Complete(r)
}
```

ç°åœ¨æˆ‘ä»¬å·²ç»äº†è§£äº†åè°ƒå™¨çš„åŸºæœ¬ç»“æ„ï¼Œè®©æˆ‘ä»¬å¡«å†™ CronJob çš„é€»è¾‘ã€‚

### å®ç°æ§åˆ¶å™¨

æºç åœ°å€ ï¼šhttps://github.com/kubernetes-sigs/kubebuilder/blob/master/docs/book/src/cronjob-tutorial/testdata/project/internal/controller/cronjob_controller.go


æˆ‘ä»¬çš„ CronJob æ§åˆ¶å™¨çš„åŸºæœ¬é€»è¾‘æ˜¯è¿™æ ·çš„ï¼š

1. åŠ è½½å‘½åçš„ CronJob
2. åˆ—å‡ºæ‰€æœ‰æ´»åŠ¨ jobsï¼Œå¹¶æ›´æ–°çŠ¶æ€
3. æ ¹æ®å†å²è®°å½•é™åˆ¶æ¸…ç†æ—§job
4. æ£€æŸ¥æˆ‘ä»¬æ˜¯å¦è¢«æš‚åœï¼ˆå¦‚æœæˆ‘ä»¬è¢«æš‚åœï¼Œè¯·ä¸è¦æ‰§è¡Œä»»ä½•å…¶ä»–æ“ä½œï¼‰
5. è·å–ä¸‹ä¸€æ¬¡scheduled run
6. å¦‚æœæ–° job æŒ‰è®¡åˆ’è¿è¡Œã€æœªè¶…è¿‡æˆªæ­¢æ—¶é—´ä¸”æœªè¢«æˆ‘ä»¬çš„å¹¶å‘ç­–ç•¥é˜»æ­¢ï¼Œåˆ™è¿è¡Œæ–° job
7. å½“æˆ‘ä»¬çœ‹åˆ°æ­£åœ¨è¿è¡Œçš„jobï¼ˆè‡ªåŠ¨å®Œæˆï¼‰æˆ–ä¸‹ä¸€æ¬¡è®¡åˆ’è¿è¡Œæ—¶é‡æ–°æ’é˜Ÿã€‚


æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ—¶é’Ÿï¼Œè¿™å°†å…è®¸æˆ‘ä»¬åœ¨æµ‹è¯•ä¸­ä¼ªé€ è®¡æ—¶ã€‚
```go
// CronJobReconciler reconciles a CronJob object
type CronJobReconciler struct {
    client.Client
    Log    logr.Logger
    Scheme *runtime.Scheme
    Clock
}
```

#### 1.æŒ‰ç…§åå­—åŠ è½½ cronjob

æˆ‘ä»¬å°†ä½¿ç”¨å®¢æˆ·ç«¯è·å– CronJobã€‚æ‰€æœ‰å®¢æˆ·ç«¯æ–¹æ³•éƒ½å°† contextï¼ˆå…è®¸å–æ¶ˆï¼‰ä½œä¸ºå…¶ç¬¬ä¸€ä¸ªå‚æ•°ï¼Œå¹¶å°†ç›¸å…³å¯¹è±¡ä½œä¸ºå…¶æœ€åä¸€ä¸ªå‚æ•°ã€‚Get æœ‰ç‚¹ç‰¹åˆ«ï¼Œå› ä¸ºå®ƒéœ€è¦ NamespacedName ä½œä¸ºå‚æ•°ã€‚

```go
    var cronJob batch.CronJob
    if err := r.Get(ctx, req.NamespacedName, &cronJob); err != nil {
        log.Error(err, "unable to fetch CronJob")
        // we'll ignore not-found errors, since they can't be fixed by an immediate
        // requeue (we'll need to wait for a new notification), and we can get them
        // on deleted requests.
        return ctrl.Result{}, client.IgnoreNotFound(err)
    }
```


#### 2.åˆ—å‡ºæ‰€æœ‰ active jobsï¼Œå¹¶ä¸”æ›´æ–° status

è‹¥è¦å®Œå…¨æ›´æ–°çŠ¶æ€ï¼Œæˆ‘ä»¬éœ€è¦åˆ—å‡ºæ­¤å‘½åç©ºé—´ä¸­å±äºæ­¤ CronJob çš„æ‰€æœ‰å­jobã€‚ä¸ Get ç±»ä¼¼ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ List æ–¹æ³•æ¥åˆ—å‡ºå­jobã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨å¯å˜å‚æ•°é€‰é¡¹æ¥è®¾ç½®å‘½åç©ºé—´å’Œå­—æ®µåŒ¹é…ï¼ˆè¿™å®é™…ä¸Šæ˜¯æˆ‘ä»¬åœ¨ä¸‹é¢è®¾ç½®çš„ç´¢å¼•æŸ¥æ‰¾ï¼‰ã€‚

```go
    var childJobs kbatch.JobList
    if err := r.List(ctx, &childJobs, client.InNamespace(req.Namespace), client.MatchingFields{jobOwnerKey: req.Name}); err != nil {
        log.Error(err, "unable to list child Jobs")
        return ctrl.Result{}, err
    }
```

æ‹¥æœ‰æˆ‘ä»¬æ‹¥æœ‰çš„æ‰€æœ‰ jobs åï¼Œæˆ‘ä»¬ä¼šå°†å®ƒä»¬æ‹†åˆ†ä¸ºæ´»åŠ¨ã€æˆåŠŸå’Œå¤±è´¥çš„ jobï¼Œè·Ÿè¸ªæœ€è¿‘çš„è¿è¡Œï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥å°†å…¶è®°å½•åœ¨çŠ¶æ€ä¸­ã€‚è¯·è®°ä½ï¼ŒçŠ¶æ€åº”è¯¥èƒ½å¤Ÿä»ä¸–ç•ŒçŠ¶æ€ä¸­é‡æ„ï¼Œå› æ­¤ä»æ ¹å¯¹è±¡çš„çŠ¶æ€è¯»å–é€šå¸¸ä¸æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚ç›¸åï¼Œæ‚¨åº”è¯¥åœ¨æ¯æ¬¡è¿è¡Œæ—¶é‡å»ºå®ƒã€‚è¿™å°±æ˜¯æˆ‘ä»¬åœ¨è¿™é‡Œè¦åšçš„ã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨çŠ¶æ€æ¡ä»¶æ£€æŸ¥jobæ˜¯å¦â€œå®Œæˆâ€ä»¥åŠå®ƒæ˜¯æˆåŠŸè¿˜æ˜¯å¤±è´¥ã€‚æˆ‘ä»¬å°†æŠŠè¿™ä¸ªé€»è¾‘æ”¾åœ¨ä¸€ä¸ªå¸®åŠ©ç¨‹åºä¸­ï¼Œä½¿æˆ‘ä»¬çš„ä»£ç æ›´å¹²å‡€ã€‚


```go
    // find the active list of jobs
    var activeJobs []*kbatch.Job
    var successfulJobs []*kbatch.Job
    var failedJobs []*kbatch.Job
    var mostRecentTime *time.Time // find the last run so we can update the status

    	isJobFinished := func(job *kbatch.Job) (bool, kbatch.JobConditionType) {
		for _, c := range job.Status.Conditions {
			if (c.Type == kbatch.JobComplete || c.Type == kbatch.JobFailed) && c.Status == corev1.ConditionTrue {
				return true, c.Type
			}
		}

		return false, ""
	}
  	getScheduledTimeForJob := func(job *kbatch.Job) (*time.Time, error) {
		timeRaw := job.Annotations[scheduledTimeAnnotation]
		if len(timeRaw) == 0 {
			return nil, nil
		}

		timeParsed, err := time.Parse(time.RFC3339, timeRaw)
		if err != nil {
			return nil, err
		}
		return &timeParsed, nil
	}

    for i, job := range childJobs.Items {
        _, finishedType := isJobFinished(&job)
        switch finishedType {
        case "": // ongoing
            activeJobs = append(activeJobs, &childJobs.Items[i])
        case kbatch.JobFailed:
            failedJobs = append(failedJobs, &childJobs.Items[i])
        case kbatch.JobComplete:
            successfulJobs = append(successfulJobs, &childJobs.Items[i])
        }

        // We'll store the launch time in an annotation, so we'll reconstitute that from
        // the active jobs themselves.
        scheduledTimeForJob, err := getScheduledTimeForJob(&job)
        if err != nil {
            log.Error(err, "unable to parse schedule time for child job", "job", &job)
            continue
        }
        if scheduledTimeForJob != nil {
            if mostRecentTime == nil {
                mostRecentTime = scheduledTimeForJob
            } else if mostRecentTime.Before(*scheduledTimeForJob) {
                mostRecentTime = scheduledTimeForJob
            }
        }
    }

    if mostRecentTime != nil {
        cronJob.Status.LastScheduleTime = &metav1.Time{Time: *mostRecentTime}
    } else {
        cronJob.Status.LastScheduleTime = nil
    }
    cronJob.Status.Active = nil
    for _, activeJob := range activeJobs {
        jobRef, err := ref.GetReference(r.Scheme, activeJob)
        if err != nil {
            log.Error(err, "unable to make reference to active job", "job", activeJob)
            continue
        }
        cronJob.Status.Active = append(cronJob.Status.Active, *jobRef)
    }
```    


åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è®°å½•æˆ‘ä»¬åœ¨ç¨é«˜çš„æ—¥å¿—è®°å½•çº§åˆ«è§‚å¯Ÿåˆ°çš„jobæ•°é‡ï¼Œä»¥ä¾¿è¿›è¡Œè°ƒè¯•ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨æ ¼å¼å­—ç¬¦ä¸²ï¼Œè€Œæ˜¯ä½¿ç”¨å›ºå®šæ¶ˆæ¯ï¼Œå¹¶å°†é”®å€¼å¯¹ä¸é¢å¤–ä¿¡æ¯é™„åŠ åœ¨ä¸€èµ·ã€‚è¿™æ ·å¯ä»¥æ›´è½»æ¾åœ°ç­›é€‰å’ŒæŸ¥è¯¢æ—¥å¿—è¡Œã€‚

```go
    log.V(1).Info("job count", "active jobs", len(activeJobs), "successful jobs", len(successfulJobs), "failed jobs", len(failedJobs))
```    

ä½¿ç”¨æˆ‘ä»¬æ”¶é›†çš„æ—¥æœŸï¼Œæˆ‘ä»¬å°†æ›´æ–° CRD çš„çŠ¶æ€ã€‚å°±åƒä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„å®¢æˆ·ã€‚ä¸ºäº†ä¸“é—¨æ›´æ–°çŠ¶æ€å­èµ„æºï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®¢æˆ·ç«¯çš„ Status éƒ¨åˆ†å’Œ Update æ–¹æ³•ã€‚


```go
//çŠ¶æ€å­èµ„æºå¿½ç•¥å¯¹è§„èŒƒçš„æ›´æ”¹ï¼Œå› æ­¤å®ƒä¸å¤ªå¯èƒ½ä¸ä»»ä½•å…¶ä»–æ›´æ–°å†²çªï¼Œå¹¶ä¸”å¯ä»¥å…·æœ‰å•ç‹¬çš„æƒé™ã€‚
    if err := r.Status().Update(ctx, &cronJob); err != nil {
        log.Error(err, "unable to update CronJob status")
        return ctrl.Result{}, err
    }
```

æ›´æ–°çŠ¶æ€åï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­ç¡®ä¿ä¸–ç•Œçš„çŠ¶æ€ç¬¦åˆspecä¸­çš„è¦æ±‚ã€‚æœ€åï¼Œæˆ‘ä»¬å°†å°è¯•æ¸…ç†æ—§ jobï¼Œè¿™æ ·æˆ‘ä»¬å°±ä¸ä¼šç•™ä¸‹å¤ªå¤šåƒåœ¾

```go
  // NB: deleting these is "best effort" -- if we fail on a particular one,
    // we won't requeue just to finish the deleting.
    if cronJob.Spec.FailedJobsHistoryLimit != nil {
        sort.Slice(failedJobs, func(i, j int) bool {
            if failedJobs[i].Status.StartTime == nil {
                return failedJobs[j].Status.StartTime != nil
            }
            return failedJobs[i].Status.StartTime.Before(failedJobs[j].Status.StartTime)
        })
        for i, job := range failedJobs {
            if int32(i) >= int32(len(failedJobs))-*cronJob.Spec.FailedJobsHistoryLimit {
                break
            }
            if err := r.Delete(ctx, job, client.PropagationPolicy(metav1.DeletePropagationBackground)); client.IgnoreNotFound(err) != nil {
                log.Error(err, "unable to delete old failed job", "job", job)
            } else {
                log.V(0).Info("deleted old failed job", "job", job)
            }
        }
    }

    if cronJob.Spec.SuccessfulJobsHistoryLimit != nil {
        sort.Slice(successfulJobs, func(i, j int) bool {
            if successfulJobs[i].Status.StartTime == nil {
                return successfulJobs[j].Status.StartTime != nil
            }
            return successfulJobs[i].Status.StartTime.Before(successfulJobs[j].Status.StartTime)
        })
        for i, job := range successfulJobs {
            if int32(i) >= int32(len(successfulJobs))-*cronJob.Spec.SuccessfulJobsHistoryLimit {
                break
            }
            if err := r.Delete(ctx, job, client.PropagationPolicy(metav1.DeletePropagationBackground)); (err) != nil {
                log.Error(err, "unable to delete old successful job", "job", job)
            } else {
                log.V(0).Info("deleted old successful job", "job", job)
            }
        }
    }
```

#### 4.æ£€æŸ¥æˆ‘ä»¬æ˜¯å¦æš‚åœ

å¦‚æœæ­¤å¯¹è±¡å·²æŒ‚èµ·ï¼Œåˆ™æˆ‘ä»¬ä¸æƒ³è¿è¡Œä»»ä½•jobï¼Œå› æ­¤ç°åœ¨å°†åœæ­¢ã€‚å¦‚æœæˆ‘ä»¬æ­£åœ¨è¿è¡Œçš„jobå‡ºç°é—®é¢˜ï¼Œå¹¶ä¸”æˆ‘ä»¬å¸Œæœ›æš‚åœè¿è¡Œä»¥è°ƒæŸ¥æˆ–æ”¾ç½®é›†ç¾¤ï¼Œè€Œä¸åˆ é™¤å¯¹è±¡ï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

```go
    if cronJob.Spec.Suspend != nil && *cronJob.Spec.Suspend {
        log.V(1).Info("cronjob suspended, skipping")
        return ctrl.Result{}, nil
    }
```    

#### 5.è·å–ä¸‹ä¸€æ¬¡ scheduled run

å¦‚æœæˆ‘ä»¬æ²¡æœ‰æš‚åœï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—ä¸‹ä¸€ä¸ªè®¡åˆ’çš„è¿è¡Œï¼Œä»¥åŠæˆ‘ä»¬æ˜¯å¦æœ‰å°šæœªå¤„ç†çš„è¿è¡Œã€‚

```go
//getNextSchedule
    // figure out the next times that we need to create
    // jobs at (or anything we missed).
    missedRun, nextRun, err := getNextSchedule(&cronJob, r.Now())
    if err != nil {
        log.Error(err, "unable to figure out CronJob schedule")
        // we don't really care about requeuing until we get an update that
        // fixes the schedule, so don't return an error
        return ctrl.Result{}, nil
    }
```    

æˆ‘ä»¬å°†å‡†å¤‡æœ€ç»ˆè¯·æ±‚ä»¥é‡æ–°æ’é˜Ÿï¼Œç›´åˆ°ä¸‹ä¸€ä¸ªjobï¼Œç„¶åç¡®å®šæˆ‘ä»¬æ˜¯å¦çœŸçš„éœ€è¦è¿è¡Œã€‚

```go
    scheduledResult := ctrl.Result{RequeueAfter: nextRun.Sub(r.Now())} // save this so we can re-use it elsewhere
    log = log.WithValues("now", r.Now(), "next run", nextRun)
```    

####  6ï¼šå¦‚æœæ–°jobæŒ‰è®¡åˆ’è¿è¡Œã€æœªè¶…è¿‡æˆªæ­¢æ—¶é—´ä¸”æœªè¢«æˆ‘ä»¬çš„å¹¶å‘ç­–ç•¥é˜»æ­¢ï¼Œåˆ™è¿è¡Œæ–°job

å¦‚æœæˆ‘ä»¬é”™è¿‡äº†ä¸€æ¬¡runï¼Œå¹¶ä¸”æˆ‘ä»¬ä»åœ¨å¼€å§‹å®ƒçš„æˆªæ­¢æ—¥æœŸå†…ï¼Œæˆ‘ä»¬å°†éœ€è¦è¿è¡Œä¸€ä¸ªjobã€‚


```go
    if missedRun.IsZero() {
        log.V(1).Info("no upcoming scheduled times, sleeping until next")
        return scheduledResult, nil
    }

    // make sure we're not too late to start the run
    log = log.WithValues("current run", missedRun)
    tooLate := false
    if cronJob.Spec.StartingDeadlineSeconds != nil {
        tooLate = missedRun.Add(time.Duration(*cronJob.Spec.StartingDeadlineSeconds) * time.Second).Before(r.Now())
    }
    if tooLate {
        log.V(1).Info("missed starting deadline for last run, sleeping till next")
        // TODO(directxman12): events
        return scheduledResult, nil
    }
```    

å¦‚æœæˆ‘ä»¬å®é™…ä¸Šå¿…é¡»è¿è¡Œä¸€ä¸ªjobï¼Œæˆ‘ä»¬éœ€è¦ç­‰åˆ°ç°æœ‰çš„jobå®Œæˆï¼Œæ›¿æ¢ç°æœ‰çš„jobï¼Œæˆ–è€…åªæ˜¯æ·»åŠ æ–°çš„jobã€‚å¦‚æœæˆ‘ä»¬çš„ä¿¡æ¯ç”±äºç¼“å­˜å»¶è¿Ÿè€Œè¿‡æœŸï¼Œæˆ‘ä»¬å°†åœ¨è·å–æœ€æ–°ä¿¡æ¯æ—¶é‡æ–°æ’é˜Ÿã€‚

```go
    // figure out how to run this job -- concurrency policy might forbid us from running
    // multiple at the same time...
    if cronJob.Spec.ConcurrencyPolicy == batch.ForbidConcurrent && len(activeJobs) > 0 {
        log.V(1).Info("concurrency policy blocks concurrent runs, skipping", "num active", len(activeJobs))
        return scheduledResult, nil
    }

    // ...or instruct us to replace existing ones...
    if cronJob.Spec.ConcurrencyPolicy == batch.ReplaceConcurrent {
        for _, activeJob := range activeJobs {
            // we don't care if the job was already deleted
            if err := r.Delete(ctx, activeJob, client.PropagationPolicy(metav1.DeletePropagationBackground)); client.IgnoreNotFound(err) != nil {
                log.Error(err, "unable to delete active job", "job", activeJob)
                return ctrl.Result{}, err
            }
        }
    }
```  

ä¸€æ—¦æˆ‘ä»¬å¼„æ¸…æ¥šå¦‚ä½•å¤„ç†ç°æœ‰ jobï¼Œæˆ‘ä»¬å®é™…ä¸Šå°±ä¼šåˆ›å»ºæˆ‘ä»¬æƒ³è¦çš„jobã€‚


#### 7ï¼šå½“æˆ‘ä»¬çœ‹åˆ°æ­£åœ¨è¿è¡Œçš„jobæˆ–ä¸‹ä¸€æ¬¡è®¡åˆ’è¿è¡Œæ—¶é‡æ–°æ’é˜Ÿ

æœ€åï¼Œæˆ‘ä»¬å°†è¿”å›ä¸Šé¢å‡†å¤‡çš„ç»“æœï¼Œå³æˆ‘ä»¬å¸Œæœ›åœ¨ä¸‹ä¸€æ¬¡è¿è¡Œéœ€è¦æ—¶é‡æ–°æ’é˜Ÿã€‚è¿™è¢«è§†ä¸ºæœ€å¤§æˆªæ­¢æ—¥æœŸ - å¦‚æœä¸¤è€…ä¹‹é—´å‘ç”Ÿäº†å…¶ä»–å˜åŒ–ï¼Œä¾‹å¦‚æˆ‘ä»¬çš„å·¥ä½œå¼€å§‹æˆ–ç»“æŸï¼Œæˆ‘ä»¬è¢«ä¿®æ”¹ç­‰ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šæ›´å¿«åœ°å†æ¬¡åè°ƒã€‚

```go
    // we'll requeue once we see the running job, and update our status
    return scheduledResult, nil

```

#### 8.Setup è®¾ç½®

æœ€åï¼Œæˆ‘ä»¬å°†æ›´æ–°æˆ‘ä»¬çš„è®¾ç½®ã€‚ä¸ºäº†è®©åè°ƒç¨‹åºèƒ½å¤ŸæŒ‰å…¶æ‰€æœ‰è€…å¿«é€ŸæŸ¥æ‰¾jobï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªç´¢å¼•ã€‚æˆ‘ä»¬å£°æ˜ä¸€ä¸ªç´¢å¼•é”®ï¼Œç¨åå¯ä»¥å°†å…¶ç”¨ä½œä¼ªå­—æ®µåç§°ï¼Œç„¶åæè¿°å¦‚ä½•ä» Job å¯¹è±¡ä¸­æå–ç´¢å¼•å€¼ã€‚ç´¢å¼•å™¨å°†è‡ªåŠ¨ä¸ºæˆ‘ä»¬å¤„ç†å‘½åç©ºé—´ï¼Œå› æ­¤ï¼Œå¦‚æœjobå…·æœ‰ CronJob æ‰€æœ‰è€…ï¼Œæˆ‘ä»¬åªéœ€è¦æå–æ‰€æœ‰è€…åç§°ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬å°†é€šçŸ¥ç®¡ç†å™¨æ­¤æ§åˆ¶å™¨æ‹¥æœ‰ä¸€äº›jobï¼Œä»¥ä¾¿åœ¨jobæ›´æ”¹ã€åˆ é™¤ç­‰æ—¶è‡ªåŠ¨è°ƒç”¨åŸºç¡€ CronJob ä¸Šçš„åè°ƒã€‚

```go
var (
    jobOwnerKey = ".metadata.controller"
    apiGVStr    = batch.GroupVersion.String()
)

func (r *CronJobReconciler) SetupWithManager(mgr ctrl.Manager) error {
    // set up a real clock, since we're not in a test
    if r.Clock == nil {
        r.Clock = realClock{}
    }

    if err := mgr.GetFieldIndexer().IndexField(&kbatch.Job{}, jobOwnerKey, func(rawObj runtime.Object) []string {
        // grab the job object, extract the owner...
        job := rawObj.(*kbatch.Job)
        owner := metav1.GetControllerOf(job)
        if owner == nil {
            return nil
        }
        // ...make sure it's a CronJob...
        if owner.APIVersion != apiGVStr || owner.Kind != "CronJob" {
            return nil
        }

        // ...and if so, return it
        return []string{owner.Name}
    }); err != nil {
        return err
    }

    return ctrl.NewControllerManagedBy(mgr).
        For(&batch.CronJob{}).
        Owns(&kbatch.Job{}).
        Complete(r)
}
```

